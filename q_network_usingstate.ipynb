{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except ImportError:\n",
    "    import _pickle as pickle\n",
    "import copy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_features = ['MIMICzs1','MIMICzs2','MIMICzs3','MIMICzs4','MIMICzs5','MIMICzs6','MIMICzs7','MIMICzs8','MIMICzs9','MIMICzs10','MIMICzs11','MIMICzs12','MIMICzs13','MIMICzs14','MIMICzs15','MIMICzs16','MIMICzs17','MIMICzs18','MIMICzs19','MIMICzs20','MIMICzs21','MIMICzs22','MIMICzs23','MIMICzs24','MIMICzs25','MIMICzs26','MIMICzs27','MIMICzs28','MIMICzs29','MIMICzs30','MIMICzs31','MIMICzs32','MIMICzs33','MIMICzs34','MIMICzs35','MIMICzs36','MIMICzs37','MIMICzs38','MIMICzs39','MIMICzs40','MIMICzs41','MIMICzs42','MIMICzs43','MIMICzs44','MIMICzs45','MIMICzs46','MIMICzs47', 'abchange_vc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(state_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data_vasochange/train_withterm.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bloc</th>\n",
       "      <th>icustayid</th>\n",
       "      <th>mortality_90d</th>\n",
       "      <th>MIMICzs1</th>\n",
       "      <th>MIMICzs2</th>\n",
       "      <th>MIMICzs3</th>\n",
       "      <th>MIMICzs4</th>\n",
       "      <th>MIMICzs5</th>\n",
       "      <th>MIMICzs6</th>\n",
       "      <th>MIMICzs7</th>\n",
       "      <th>...</th>\n",
       "      <th>MIMICzs44</th>\n",
       "      <th>MIMICzs45</th>\n",
       "      <th>MIMICzs46</th>\n",
       "      <th>MIMICzs47</th>\n",
       "      <th>action</th>\n",
       "      <th>shaped_reward</th>\n",
       "      <th>io_ac</th>\n",
       "      <th>vc_ac</th>\n",
       "      <th>abchange_vc</th>\n",
       "      <th>max_dose_vaso</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-2.302585</td>\n",
       "      <td>-0.978344</td>\n",
       "      <td>-0.187300</td>\n",
       "      <td>0.705956</td>\n",
       "      <td>...</td>\n",
       "      <td>0.391651</td>\n",
       "      <td>0.52761</td>\n",
       "      <td>0.786192</td>\n",
       "      <td>0.702781</td>\n",
       "      <td>10</td>\n",
       "      <td>-1.224270</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-2.302585</td>\n",
       "      <td>-0.978344</td>\n",
       "      <td>-0.187300</td>\n",
       "      <td>0.705956</td>\n",
       "      <td>...</td>\n",
       "      <td>0.394176</td>\n",
       "      <td>0.52761</td>\n",
       "      <td>0.793676</td>\n",
       "      <td>0.596530</td>\n",
       "      <td>10</td>\n",
       "      <td>1.640796</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-2.302585</td>\n",
       "      <td>-0.978344</td>\n",
       "      <td>-0.187300</td>\n",
       "      <td>0.705956</td>\n",
       "      <td>...</td>\n",
       "      <td>0.396682</td>\n",
       "      <td>0.52761</td>\n",
       "      <td>0.799286</td>\n",
       "      <td>0.516950</td>\n",
       "      <td>10</td>\n",
       "      <td>-0.025000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-2.302585</td>\n",
       "      <td>-0.978344</td>\n",
       "      <td>-0.155313</td>\n",
       "      <td>0.705956</td>\n",
       "      <td>...</td>\n",
       "      <td>0.399169</td>\n",
       "      <td>0.52761</td>\n",
       "      <td>0.805952</td>\n",
       "      <td>0.575231</td>\n",
       "      <td>10</td>\n",
       "      <td>-0.025000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-2.302585</td>\n",
       "      <td>-0.978344</td>\n",
       "      <td>-0.147317</td>\n",
       "      <td>0.705956</td>\n",
       "      <td>...</td>\n",
       "      <td>0.401637</td>\n",
       "      <td>0.52761</td>\n",
       "      <td>0.816227</td>\n",
       "      <td>0.714111</td>\n",
       "      <td>10</td>\n",
       "      <td>-0.025000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   bloc  icustayid  mortality_90d  MIMICzs1  MIMICzs2  MIMICzs3  MIMICzs4  \\\n",
       "0     1          3              1      -0.5      -0.5      -0.5 -2.302585   \n",
       "1     2          3              1      -0.5      -0.5      -0.5 -2.302585   \n",
       "2     3          3              1      -0.5      -0.5      -0.5 -2.302585   \n",
       "3     4          3              1      -0.5      -0.5      -0.5 -2.302585   \n",
       "4     5          3              1      -0.5      -0.5      -0.5 -2.302585   \n",
       "\n",
       "   MIMICzs5  MIMICzs6  MIMICzs7      ...        MIMICzs44  MIMICzs45  \\\n",
       "0 -0.978344 -0.187300  0.705956      ...         0.391651    0.52761   \n",
       "1 -0.978344 -0.187300  0.705956      ...         0.394176    0.52761   \n",
       "2 -0.978344 -0.187300  0.705956      ...         0.396682    0.52761   \n",
       "3 -0.978344 -0.155313  0.705956      ...         0.399169    0.52761   \n",
       "4 -0.978344 -0.147317  0.705956      ...         0.401637    0.52761   \n",
       "\n",
       "   MIMICzs46  MIMICzs47  action  shaped_reward  io_ac  vc_ac  abchange_vc  \\\n",
       "0   0.786192   0.702781      10      -1.224270      2      0          0.0   \n",
       "1   0.793676   0.596530      10       1.640796      2      0          0.0   \n",
       "2   0.799286   0.516950      10      -0.025000      2      0          0.0   \n",
       "3   0.805952   0.575231      10      -0.025000      2      0          0.0   \n",
       "4   0.816227   0.714111      10      -0.025000      2      0          0.0   \n",
       "\n",
       "   max_dose_vaso  \n",
       "0            0.0  \n",
       "1            0.0  \n",
       "2            0.0  \n",
       "3            0.0  \n",
       "4            0.0  \n",
       "\n",
       "[5 rows x 56 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Uncomment for teesting and validation data\n",
    "val_df = pd.read_csv('data_vasochange/val_withterm.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('data_vasochange/test_withterm.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "REWARD_THRESHOLD = 20\n",
    "reg_lambda = 5\n",
    "reg_lambda2 = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PER important weights and params\n",
    "per_flag = True\n",
    "beta_start = 0.9\n",
    "df['prob'] = abs(df['shaped_reward'])\n",
    "temp = 1.0/df['prob']\n",
    "#temp[temp == float('Inf')] = 1.0\n",
    "df['imp_weight'] = pow((1.0/len(df) * temp), beta_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET THIS TO FALSE\n",
    "clip_reward = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_dose_map = {0: 0, 1: 0.04, 2: 0.135, 3: 0.27, 4:0.7859999999999999, 5: 0, 6: 0.04, 7: 0.135, 8:0.27, 9:0.7859999999999999, 10: 0, 11:0.04, 12: 0.135, 13:0.27, 14:0.7859999999999999, 15:0, 16:0.04, 17:0.135, 18:0.27, 19:0.7859999999999999, 20:0, 21:0.04, 22: 0.135, 23:0.27, 24: 0.7859999999999999}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_1_size = 128\n",
    "hidden_2_size = 128\n",
    "#  Q-network uses Leaky ReLU activation\n",
    "class Qnetwork():\n",
    "    def __init__(self):\n",
    "        self.phase = tf.placeholder(tf.bool)\n",
    "\n",
    "        self.num_actions = 25\n",
    "\n",
    "        self.input_size = len(state_features)\n",
    "\n",
    "        self.state = tf.placeholder(tf.float32, shape=[None, self.input_size],name=\"input_state\")\n",
    "\n",
    "        self.fc_1 = tf.contrib.layers.fully_connected(self.state, hidden_1_size, activation_fn=None)\n",
    "        self.fc_1_bn = tf.contrib.layers.batch_norm(self.fc_1, center=True, scale=True, is_training=self.phase)\n",
    "        self.fc_1_ac = tf.maximum(self.fc_1_bn, self.fc_1_bn*0.01)\n",
    "        self.fc_2 = tf.contrib.layers.fully_connected(self.fc_1_ac, hidden_2_size, activation_fn=None)\n",
    "        self.fc_2_bn = tf.contrib.layers.batch_norm(self.fc_2, center=True, scale=True, is_training=self.phase)\n",
    "        self.fc_2_ac = tf.maximum(self.fc_2_bn, self.fc_2_bn*0.01)\n",
    "        \n",
    "        # advantage and value streams\n",
    "        self.streamA,self.streamV = tf.split(self.fc_2_ac,2,axis=1)\n",
    "        self.AW = tf.Variable(tf.random_normal([hidden_2_size//2,self.num_actions]))\n",
    "        self.VW = tf.Variable(tf.random_normal([hidden_2_size//2,1]))\n",
    "        self.Advantage = tf.matmul(self.streamA,self.AW)\n",
    "        self.Value = tf.matmul(self.streamV,self.VW)\n",
    "        \n",
    "        #Then combine them together to get our final Q-values.\n",
    "        self.q_output = self.Value + tf.subtract(self.Advantage,tf.reduce_mean(self.Advantage,axis=1,keep_dims=True))\n",
    "       \n",
    "        self.predict = tf.argmax(self.q_output,1, name='predict') # vector of length batch size\n",
    "        \n",
    "        #Below we obtain the loss by taking the sum of squares difference between the target and predicted Q values.\n",
    "        self.targetQ = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "        self.actions = tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "        self.actions_onehot = tf.one_hot(self.actions,self.num_actions,dtype=tf.float32)\n",
    "        \n",
    "        # Importance sampling weights for PER, used in network update         \n",
    "        self.imp_weights = tf.placeholder(shape=[None], dtype=tf.float32)\n",
    "        \n",
    "        # select the Q values for the actions that would be selected         \n",
    "        self.Q = tf.reduce_sum(tf.multiply(self.q_output, self.actions_onehot), reduction_indices=1) # batch size x 1 vector\n",
    "        \n",
    "        \n",
    "        # regularisation penalises the network when it produces rewards that are above the\n",
    "        # reward threshold, to ensure reasonable Q-value predictions      \n",
    "        self.reg_vector = tf.maximum(tf.abs(self.Q)-REWARD_THRESHOLD,0)\n",
    "        self.reg_term = tf.reduce_sum(self.reg_vector)\n",
    "        \n",
    "        \n",
    "        ############## second regularisation term ###################################\n",
    "        \n",
    "        self.pre_vasodose = tf.exp(self.state[:,3])- 0.1 -self.state[:,47]\n",
    "        \n",
    "        self.cur_vasodose = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "        \n",
    "        #self.cur_vasodose = np.array([action_dose_map[i] for i in self.predict], dtype= np.float32)\n",
    "        self.tf_abchange = tf.minimum(tf.abs(tf.subtract(self.cur_vasodose, self.pre_vasodose)), 0.785)\n",
    "        \n",
    "        self.reg_vector2 = tf.where(self.tf_abchange >= 0.785, tf.ones_like(self.tf_abchange), tf.zeros_like(self.tf_abchange)) # shape=(?,)\n",
    "        self.reg_term2 = tf.reduce_sum(self.reg_vector2)\n",
    "        \n",
    "        \n",
    "        ##############################################################################\n",
    "        \n",
    "        self.abs_error = tf.abs(self.targetQ - self.Q)\n",
    "        \n",
    "        self.td_error = tf.square(self.targetQ - self.Q)\n",
    "        \n",
    "        # below is the loss when we are not using PER\n",
    "        self.old_loss = tf.reduce_mean(self.td_error)\n",
    "        \n",
    "        # as in the paper, to get PER loss we weight the squared error by the importance weights\n",
    "        self.per_error = tf.multiply(self.td_error, self.imp_weights)\n",
    "\n",
    "        # total loss is a sum of PER loss and the regularisation term\n",
    "        if per_flag:\n",
    "            self.loss = tf.reduce_mean(self.per_error) + reg_lambda*self.reg_term + reg_lambda2*self.reg_term2\n",
    "        else:\n",
    "            self.loss = self.old_loss + reg_lambda*self.reg_term\n",
    "\n",
    "        self.trainer = tf.train.AdamOptimizer(learning_rate=0.0001)\n",
    "        self.update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(self.update_ops):\n",
    "        # Ensures that we execute the update_ops before performing the model update, so batchnorm works\n",
    "            self.update_model = self.trainer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function is needed to update parameters between main and target network\n",
    "# tf_vars are the trainable variables to update, and tau is the rate at which to update\n",
    "# returns tf ops corresponding to the updates\n",
    "def update_target_graph(tf_vars,tau):\n",
    "    total_vars = len(tf_vars)\n",
    "    op_holder = []\n",
    "    for idx,var in enumerate(tf_vars[0:int(total_vars/2)]):\n",
    "        op_holder.append(tf_vars[idx+int(total_vars/2)].assign((var.value()*tau) + ((1-tau)*tf_vars[idx+int(total_vars/2)].value())))\n",
    "    return op_holder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_target(op_holder,sess):\n",
    "    for op in op_holder:\n",
    "        sess.run(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_train_batch(size):\n",
    "    if per_flag:\n",
    "        # uses prioritised exp replay\n",
    "        a = df.sample(n=size, weights=df['prob'])\n",
    "    else:\n",
    "        a = df.sample(n=size)\n",
    "    states = None\n",
    "    actions = None\n",
    "    rewards = None\n",
    "    next_states = None\n",
    "    done_flags = None\n",
    "    for i in a.index:\n",
    "        cur_state = a.ix[i,state_features]\n",
    "        #iv = int(a.ix[i, 'iv_input'])\n",
    "        #vaso = int(a.ix[i, 'vaso_input'])\n",
    "        #action = action_map[iv,vaso]\n",
    "        action = int(a.ix[i, 'action'])\n",
    "        reward = a.ix[i,'shaped_reward']\n",
    "        \n",
    "        if clip_reward:\n",
    "            if reward > 1: reward = 1\n",
    "            if reward < -1: reward = -1\n",
    "\n",
    "        if i != df.index[-1]:\n",
    "            # if not terminal step in trajectory             \n",
    "            if df.ix[i, 'icustayid'] == df.ix[i+1, 'icustayid']:\n",
    "                next_state = df.ix[i + 1, state_features]\n",
    "                done = 0\n",
    "            else:\n",
    "                # trajectory is finished\n",
    "                next_state = np.zeros(len(cur_state))\n",
    "                done = 1\n",
    "        else:\n",
    "            # last entry in df is the final state of that trajectory\n",
    "            next_state = np.zeros(len(cur_state))\n",
    "            done = 1\n",
    "\n",
    "        if states is None:\n",
    "            states = copy.deepcopy(cur_state)\n",
    "        else:\n",
    "            states = np.vstack((states,cur_state))\n",
    "\n",
    "        if actions is None:\n",
    "            actions = [action]\n",
    "        else:\n",
    "            actions = np.vstack((actions,action))\n",
    "\n",
    "        if rewards is None:\n",
    "            rewards = [reward]\n",
    "        else:\n",
    "            rewards = np.vstack((rewards,reward))\n",
    "\n",
    "        if next_states is None:\n",
    "            next_states = copy.deepcopy(next_state)\n",
    "        else:\n",
    "            next_states = np.vstack((next_states,next_state))\n",
    "\n",
    "        if done_flags is None:\n",
    "            done_flags = [done]\n",
    "        else:\n",
    "            done_flags = np.vstack((done_flags,done))\n",
    "    \n",
    "    return (states, np.squeeze(actions), np.squeeze(rewards), next_states, np.squeeze(done_flags), a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract chunks of length size from the relevant dataframe, and yield these to the caller\n",
    "def process_eval_batch(size, eval_type = None):\n",
    "    if eval_type is None:\n",
    "        raise Exception('Provide eval_type to process_eval_batch')\n",
    "    elif eval_type == 'train':\n",
    "        a = df.copy()\n",
    "    elif eval_type == 'val':\n",
    "        a = val_df.copy()\n",
    "    elif eval_type == 'test':\n",
    "        a = test_df.copy()\n",
    "    else:\n",
    "        raise Exception('Unknown eval_type')\n",
    "    count = 0\n",
    "    while count < len(a.index):\n",
    "       # print(count, len(a.index))\n",
    "        states = None\n",
    "        actions = None\n",
    "        rewards = None\n",
    "        next_states = None\n",
    "        done_flags = None\n",
    "\n",
    "        start_idx = count\n",
    "        end_idx = min(len(a.index), count+size)\n",
    "        segment = a.index[start_idx:end_idx]\n",
    "        \n",
    "        for i in segment:\n",
    "            cur_state = a.ix[i,state_features]\n",
    "            #iv = int(a.ix[i, 'iv_input'])\n",
    "            #vaso = int(a.ix[i, 'vaso_input'])\n",
    "            #action = action_map[iv,vaso]\n",
    "            action = int(a.ix[i,'action'])\n",
    "            reward = a.ix[i,'shaped_reward']\n",
    "            \n",
    "            if clip_reward:\n",
    "                if reward > 1: reward = 1\n",
    "                if reward < -1: reward = -1\n",
    "\n",
    "            if i != a.index[-1]:\n",
    "                # if not terminal step in trajectory             \n",
    "                if a.ix[i, 'icustayid'] == a.ix[i+1, 'icustayid']:\n",
    "                    next_state = a.ix[i + 1, state_features]\n",
    "                    done = 0\n",
    "                else:\n",
    "                    # trajectory is finished\n",
    "                    next_state = np.zeros(len(cur_state))\n",
    "                    done = 1\n",
    "            else:\n",
    "                # last entry in df is the final state of that trajectory\n",
    "                next_state = np.zeros(len(cur_state))\n",
    "                done = 1\n",
    "\n",
    "            if states is None:\n",
    "                states = copy.deepcopy(cur_state)\n",
    "            else:\n",
    "                states = np.vstack((states,cur_state))\n",
    "\n",
    "            if actions is None:\n",
    "                actions = [action]\n",
    "            else:\n",
    "                actions = np.vstack((actions,action))\n",
    "\n",
    "            if rewards is None:\n",
    "                rewards = [reward]\n",
    "            else:\n",
    "                rewards = np.vstack((rewards,reward))\n",
    "\n",
    "            if next_states is None:\n",
    "                next_states = copy.deepcopy(next_state)\n",
    "            else:\n",
    "                next_states = np.vstack((next_states,next_state))\n",
    "\n",
    "            if done_flags is None:\n",
    "                done_flags = [done]\n",
    "            else:\n",
    "                done_flags = np.vstack((done_flags,done))\n",
    "\n",
    "        yield (states, np.squeeze(actions), np.squeeze(rewards), next_states, np.squeeze(done_flags), a)\n",
    "        \n",
    "        count += size\n",
    "       # if count >= 3000:\n",
    "       #     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_eval(eval_type):\n",
    "    gen = process_eval_batch(size = 1000, eval_type=eval_type)\n",
    "\n",
    "    phys_q_ret = []\n",
    "    actions_ret = []\n",
    "    agent_q_ret = []\n",
    "    actions_taken_ret = []\n",
    "    error_ret = 0\n",
    "  \n",
    "\n",
    "    for b in gen:\n",
    "        \n",
    "        states,actions,rewards,next_states, done_flags, _ = b\n",
    "\n",
    "        # firstly get the chosen actions at the next timestep\n",
    "        actions_from_q1 = sess.run(mainQN.predict,feed_dict={mainQN.state:next_states, mainQN.phase : 0})\n",
    "\n",
    "        # Q values for the next timestep from target network, as part of the Double DQN update\n",
    "        Q2 = sess.run(targetQN.q_output,feed_dict={targetQN.state:next_states, targetQN.phase : 0})\n",
    "\n",
    "        # handles the case when a trajectory is finished\n",
    "        end_multiplier = 1 - done_flags\n",
    "\n",
    "        # target Q value using Q values from target, and actions from main\n",
    "        double_q_value = Q2[range(len(Q2)),actions_from_q1]\n",
    "\n",
    "        # definition of target Q\n",
    "        targetQ = rewards + (gamma*double_q_value * end_multiplier)\n",
    "\n",
    "        # get the output q's, actions, and loss\n",
    "        q_output,actions_taken, abs_error = sess.run([mainQN.q_output,mainQN.predict, mainQN.abs_error], \\\n",
    "            feed_dict={mainQN.state:states,\n",
    "                       mainQN.targetQ:targetQ, \n",
    "                       mainQN.actions:actions,\n",
    "                       mainQN.phase:False})\n",
    "        #print(actions_taken)\n",
    "       # actions_taken=actions_taken-1\n",
    "        # return the relevant q values and actions\n",
    "        phys_q = q_output[range(len(q_output)), actions]\n",
    "        agent_q = q_output[range(len(q_output)), actions_taken]\n",
    "        error = np.mean(abs_error)\n",
    "        \n",
    "#       update the return vals\n",
    "        phys_q_ret.extend(phys_q)\n",
    "\n",
    "        actions_ret.extend(actions)        \n",
    "        agent_q_ret.extend(agent_q)\n",
    "        actions_taken_ret.extend(actions_taken)\n",
    "        error_ret += error\n",
    "\n",
    "    #print(error_ret)\n",
    "    return phys_q_ret, actions_ret, agent_q_ret, actions_taken_ret, error_ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Used to run diagnostics on the train set\n",
    "phys_q_train = []\n",
    "agent_q_train = []\n",
    "phys_actions_tr = []\n",
    "agent_actions_tr = []\n",
    "def train_set_performance():\n",
    "    count = 0\n",
    "    global phys_q_train\n",
    "    global agent_q_train\n",
    "    global phys_actions\n",
    "    global agent_actions\n",
    "    phys_q_train = []\n",
    "    agent_q_train = []\n",
    "    phys_actions_tr = []\n",
    "    agent_actions_tr = []\n",
    "    for r in df.index:\n",
    "        cur_state = [df.ix[r,state_features]]\n",
    "        #iv = int(df.ix[r, 'iv_input'])\n",
    "        #vaso = int(df.ix[r, 'vaso_input'])\n",
    "        #action = action_map[iv,vaso]\n",
    "        action = int(df.ix[r,'action'])\n",
    "        output_q = np.squeeze(sess.run(mainQN.q_output, feed_dict = {mainQN.state : cur_state, mainQN.phase : False}))\n",
    "        phys_q_train.append(output_q[action])\n",
    "        agent_q_train.append(max(output_q))\n",
    "        agent_actions_tr.append(np.argmax(output_q))\n",
    "        phys_actions_tr.append(action)\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True  # Don't use all GPUs \n",
    "config.allow_soft_placement = True  # Enable manual control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_save_results():\n",
    "    # get the chosen actions for the train, val, and test set when training is complete.\n",
    "    _, _, agent_q_train, agent_actions_train, _ = do_eval(eval_type = 'train')\n",
    "    print (\"length IS \", len(agent_actions_train))\n",
    "    with open(save_dir + 'dqn_normal_actions_train.p', 'wb') as f:\n",
    "        pickle.dump(agent_actions_train, f)\n",
    "    _, _, agent_q_val, agent_actions_val, _ = do_eval(eval_type = 'val')        \n",
    "    _, _, agent_q_test, agent_actions_test, _ = do_eval(eval_type = 'test')   \n",
    "    \n",
    "    # save everything for later - they're used in policy evaluation and when generating plots\n",
    "    with open(save_dir + 'dqn_normal_actions_train.p', 'wb') as f:\n",
    "        pickle.dump(agent_actions_train, f)\n",
    "    with open(save_dir + 'dqn_normal_actions_val.p', 'wb') as f:\n",
    "        pickle.dump(agent_actions_val, f)\n",
    "    with open(save_dir + 'dqn_normal_actions_test.p', 'wb') as f:\n",
    "        pickle.dump(agent_actions_test, f)\n",
    "        \n",
    "    with open(save_dir + 'dqn_normal_q_train.p', 'wb') as f:\n",
    "        pickle.dump(agent_q_train, f)\n",
    "    with open(save_dir + 'dqn_normal_q_val.p', 'wb') as f:\n",
    "        pickle.dump(agent_q_val, f)\n",
    "    with open(save_dir + 'dqn_normal_q_test.p', 'wb') as f:\n",
    "        pickle.dump(agent_q_test, f)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-12-6cf1f6c50398>:29: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "Running default init\n",
      "Init done\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-fd9243b68706>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;31m# Calculate the importance sampling weights for PER\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0mimp_sampling_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampled_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'imp_weight'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'imp_weight'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mimp_sampling_weights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimp_sampling_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mimp_sampling_weights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimp_sampling_weights\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# The main training loop is here\n",
    "per_alpha = 0.6 # PER hyperparameter\n",
    "per_epsilon = 0.01 # PER hyperparameter\n",
    "batch_size = 32\n",
    "gamma = 0.99 # discount factor \n",
    "num_steps = 100000 # How many steps to train for\n",
    "load_model = False #Whether to load a saved model.\n",
    "save_dir = \"./vasochange4_dqn_normal/\"\n",
    "save_path = \"./vasochange4_dqn_normal/ckpt\"#The path to save our model to.\n",
    "tau = 0.001 #Rate to update target network toward primary network\n",
    "tf.reset_default_graph()\n",
    "mainQN = Qnetwork()\n",
    "targetQN = Qnetwork()\n",
    "av_q_list = []\n",
    "\n",
    "val_abserror_list = []\n",
    "abs_error_list = []\n",
    "save_results = False\n",
    "\n",
    "saver = tf.train.Saver(tf.global_variables())\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "trainables = tf.trainable_variables()\n",
    "\n",
    "target_ops = update_target_graph(trainables,tau)\n",
    "\n",
    "#Make a path for our model to be saved in.\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "    if load_model == True:\n",
    "        print('Trying to load model...')\n",
    "        try:\n",
    "            restorer = tf.train.import_meta_graph(save_path + '.meta')\n",
    "            restorer.restore(sess, tf.train.latest_checkpoint(save_dir))\n",
    "            print (\"Model restored\")\n",
    "        except IOError:\n",
    "            print (\"No previous model found, running default init\")\n",
    "            sess.run(init)\n",
    "        try:\n",
    "            per_weights = pickle.load(open( save_dir + \"per_weights.p\", \"rb\" ))\n",
    "            imp_weights = pickle.load(open( save_dir + \"imp_weights.p\", \"rb\" ))\n",
    "            \n",
    "            # the PER weights, governing probability of sampling, and importance sampling\n",
    "            # weights for use in the gradient descent updates\n",
    "            df['prob'] = per_weights\n",
    "            df['imp_weight'] = imp_weights\n",
    "            print (\"PER and Importance weights restored\")\n",
    "        except IOError:\n",
    "            print(\"No PER weights found - default being used for PER and importance sampling\")\n",
    "    else:\n",
    "        print(\"Running default init\")\n",
    "        sess.run(init)\n",
    "    print(\"Init done\")\n",
    "    \n",
    "    net_loss = 0.0\n",
    "    for i in range(num_steps):\n",
    "        if save_results:\n",
    "            print( \"Calling do save results\")\n",
    "            do_save_results()\n",
    "            break\n",
    "        \n",
    "        states,actions,rewards,next_states, done_flags, sampled_df = process_train_batch(batch_size)\n",
    "        \n",
    "        # firstly get the chosen actions at the next timestep\n",
    "        actions_from_q1 = sess.run(mainQN.predict,feed_dict={mainQN.state:next_states, mainQN.phase : 1})\n",
    "        \n",
    "        # actions chosen now, as a check\n",
    "        cur_act = sess.run(mainQN.predict,feed_dict={mainQN.state:states, mainQN.phase : 1})\n",
    "        cur_vasodose = np.array([action_dose_map[i] for i in cur_act], dtype= np.float32)\n",
    "\n",
    "        # Q values for the next timestep from target network, as part of the Double DQN update\n",
    "        Q2 = sess.run(targetQN.q_output,feed_dict={targetQN.state:next_states, targetQN.phase : 1})\n",
    "\n",
    "        # handles the case when a trajectory is finished\n",
    "        end_multiplier = 1 - done_flags\n",
    "    \n",
    "        # target Q value using Q values from target, and actions from main\n",
    "        double_q_value = Q2[range(len(Q2)),actions_from_q1]\n",
    "        \n",
    "        # empirical hack to make the Q values never exceed the threshold - helps learning\n",
    "        double_q_value[double_q_value > REWARD_THRESHOLD] = REWARD_THRESHOLD\n",
    "        double_q_value[double_q_value < -REWARD_THRESHOLD] = -REWARD_THRESHOLD\n",
    "        \n",
    "        # definition of target Q\n",
    "        targetQ = rewards + (gamma*double_q_value * end_multiplier)\n",
    "\n",
    "        # Calculate the importance sampling weights for PER\n",
    "        imp_sampling_weights = np.array(sampled_df['imp_weight'] / float(max(df['imp_weight'])))\n",
    "        imp_sampling_weights[np.isnan(imp_sampling_weights)] = 1\n",
    "        imp_sampling_weights[imp_sampling_weights <= 0.001] = 0.001\n",
    "\n",
    "        # Train with the batch\n",
    "        _,loss, error = sess.run([mainQN.update_model,mainQN.loss, mainQN.abs_error], \\\n",
    "            feed_dict={mainQN.state:states,\n",
    "                       mainQN.targetQ:targetQ, \n",
    "                       mainQN.actions:actions,\n",
    "                       mainQN.cur_vasodose:cur_vasodose,\n",
    "                       mainQN.phase:True,\n",
    "                       mainQN.imp_weights:imp_sampling_weights})\n",
    "\n",
    "        # Update target towards main network\n",
    "        update_target(target_ops,sess)\n",
    "        \n",
    "        net_loss += sum(error)\n",
    "        \n",
    "        # Set the selection weight/prob to the abs prediction error and update the importance sampling weight\n",
    "        new_weights = pow((error + per_epsilon), per_alpha)\n",
    "        df.ix[df.index.isin(sampled_df.index), 'prob'] = new_weights\n",
    "        temp = 1.0/new_weights\n",
    "        df.ix[df.index.isin(sampled_df.index), 'imp_weight'] = pow(((1.0/len(df)) * temp), beta_start)\n",
    "        \n",
    "        if i % 1000 == 0 and i > 0:\n",
    "            saver.save(sess,save_path)\n",
    "            print(\"Saved Model, step is \" + str(i))\n",
    "            \n",
    "            av_loss = net_loss/(1000.0 * batch_size)\n",
    "            abs_error_list.append(av_loss)\n",
    "            \n",
    "            print(\"Average loss is \", av_loss)\n",
    "            net_loss = 0.0\n",
    "                        \n",
    "            print (\"Saving PER and importance weights\")\n",
    "            with open(save_dir + 'per_weights.p', 'wb') as f:\n",
    "                pickle.dump(df['prob'], f)\n",
    "            with open(save_dir + 'imp_weights.p', 'wb') as f:\n",
    "                pickle.dump(df['imp_weight'], f)\n",
    "        \n",
    "#        if (i % 5000==0) and i > 0:\n",
    "            #print (\"physactions \", actions)\n",
    "            #print (\"chosen actions \", cur_act)\n",
    "            #run an evaluation on the validation set\n",
    "            phys_q, phys_actions, agent_q, agent_actions, mean_abs_error = do_eval(eval_type = 'val')            \n",
    "            val_abserror_list.append(mean_abs_error/29)  \n",
    "            \n",
    "            print (mean_abs_error/29)\n",
    "            print (np.mean(phys_q))\n",
    "            print (np.mean(agent_q))\n",
    "\n",
    "            if (i % 5000==0) and i > 0:\n",
    "                print (\"Saving results\")\n",
    "                do_save_results()\n",
    "    do_save_results()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(abs_error_list, label = 'train_loss')\n",
    "plt.plot(val_abserror_list, label = 'val_loss')\n",
    "plt.title('q network model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('train steps*1000')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
